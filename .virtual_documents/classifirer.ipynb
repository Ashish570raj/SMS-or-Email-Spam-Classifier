import pandas as pd
import numpy as np


df = pd.read_csv("spam.csv", encoding='latin1')


df.head()


df.shape


# 1. Data Cleaning
# 2. EDA
# 3. Text Preprocessing
# 4. MOdel building
# 5. Evaluation
# 6. Improvement
# 7. Website
# 8. Deploy





df.info()


# drop last 3 columns
df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace =True)


df.head()


#renaming the columns
df.rename(columns={'v1':'Target','v2':'text'},inplace=True)


df.head()


from sklearn.preprocessing import LabelEncoder
encoder=LabelEncoder()


df['Target']=encoder.fit_transform(df['Target'])


df.head()


#missing values
df.isnull().sum()


df.duplicated().sum()


df.drop_duplicates(keep='first')


df=df.drop_duplicates(keep='first')


df.duplicated().sum()


df.shape





df.head()


df['Target'].value_counts()


import matplotlib.pyplot as plt


plt.pie(df['Target'].value_counts(),labels=['ham','spam'],autopct="%0.2f")
plt.show()





import nltk


nltk.download('punkt_tab')


df['text'].apply(len)


df['num_characters']=df['text'].apply(len)


df.head()


#num of words
df['num_words']=df['text'].apply(lambda x:len(nltk.word_tokenize(x)))


df.head()


df['text'].apply(lambda x:nltk.sent_tokenize(x))


df['num_sentences']=df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))


df.head()


df[['num_characters','num_words','num_sentences']].describe()


#for ham messeges
df[df['Target']==0][['num_characters','num_words','num_sentences']].describe()


#spam
df[df['Target']==1][['num_characters','num_words','num_sentences']].describe()


import seaborn as sns


sns.histplot(df[df['Target']==0]['num_characters'])
sns.histplot(df[df['Target']==1]['num_characters'],color='red')


sns.histplot(df[df['Target']==0]['num_words'])
sns.histplot(df[df['Target']==1]['num_words'],color='red')


print(df.columns)



sns.pairplot(data=df,hue='Target')


print(df.dtypes)



# Select only numeric columns
numeric_df = df.select_dtypes(include=['int64', 'int32', 'float64'])

# Plot heatmap
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix (Numeric Features Only)")
plt.show()





def transform_text(text):
    text= text.lower()
    text= nltk.word_tokenize(text)
    return text


transform_text("HI how ARE You")


df['text']


df['text'][0]



def transform_text(text):
    text= text.lower()
    text= nltk.word_tokenize(text)
    
    y=[]
    for i in text:
        if i.isalnum():
            y.append(i)

    return y


transform_text("hi HOW are you@#")


import nltk
nltk.download('stopwords')


from nltk.corpus import stopwords
stopwords.words('english')


import string
string.punctuation


def transform_text(text):
    text= text.lower()
    text= nltk.word_tokenize(text)
    
    y=[]
    for i in text:
        if i.isalnum():
            y.append(i)

    text =y[:]
    y.clear()
    
    for i in text:
        if i not in stopwords.words('english') and i not in string.punctuation:
            y.append(i)
            
    return y


transform_text("hi how are you AshiSh ##@")


transform_text("Did you like my presentation on ML?")


df['text'][2000]


from nltk.stem.porter import PorterStemmer


ps=PorterStemmer()
ps.stem('loving')


def transform_text(text):
    text= text.lower()
    text= nltk.word_tokenize(text)
    
    y=[]
    for i in text:
        if i.isalnum():
            y.append(i)

    text =y[:]
    y.clear()
    
    for i in text:
        if i not in stopwords.words('english') and i not in string.punctuation:
            y.append(i)
            
    text=y[:]
    y.clear()
    
    for i in text:
        y.append(ps.stem(i))
            
    return " ".join(y)


transform_text("I loved the YT lectures On MAchine Learning. HOw about you?")


df['text'][0]


transform_text("'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'")


df['transformed_text']=df['text'].apply(transform_text)


df.head()


from wordcloud import WordCloud
wc= WordCloud(width=500,height=500,min_font_size=10,background_color='white')


spam_wc= wc.generate(df[df['Target']==1]['transformed_text'].str.cat(sep=' '))
plt.imshow(spam_wc)



ham_wc= wc.generate(df[df['Target']==0]['transformed_text'].str.cat(sep=' '))
plt.imshow(ham_wc)



df.head()


spam_corpus=[]
for msg in df[df['Target']==1]['transformed_text'].to_list():
    for words in msg.split():
        spam_corpus.append(words)


len(spam_corpus)


from collections import Counter
top_words_spam=Counter(spam_corpus).most_common(30)
top_words_spam


dff=pd.DataFrame(top_words_spam,columns=['words','counts'])
dff


sns.barplot(x='words', y='counts', data=dff)
plt.xticks(rotation='vertical')
plt.show()


df.head()


ham_corpus=[]
for msg in df[df['Target']==0]['transformed_text'].to_list():
    for words in msg.split():
        ham_corpus.append(words)


len(ham_corpus)


from collections import Counter
top_words_ham=Counter(ham_corpus).most_common(30)
top_words_ham


dfff=pd.DataFrame(top_words_ham,columns=['words','counts'])
dfff


sns.barplot(x='words', y='counts', data=dfff)
plt.xticks(rotation='vertical')
plt.show()








df.head()


from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
cv= CountVectorizer()
tfidf=TfidfVectorizer(max_features=3000)


x= cv.fit_transform(df['transformed_text']).toarray()


x.shape


y=df['Target'].values


y


from sklearn.model_selection import train_test_split


x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.2,random_state=2)


from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score


gnb=GaussianNB()
mnb=MultinomialNB()
bnb=BernoulliNB()


gnb.fit(x_train,y_train)
y_pred1=gnb.predict(x_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))


mnb.fit(x_train,y_train)
y_pred2=mnb.predict(x_test)
print(accuracy_score(y_test,y_pred2))
print(confusion_matrix(y_test,y_pred2))
print(precision_score(y_test,y_pred2))


bnb.fit(x_train,y_train)
y_pred3=bnb.predict(x_test)
print(accuracy_score(y_test,y_pred3))
print(confusion_matrix(y_test,y_pred3))
print(precision_score(y_test,y_pred3))


from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier


# Creating classifier objects with parameters
svc = SVC(kernel='sigmoid', gamma=1.0)
knc = KNeighborsClassifier()
mnb = MultinomialNB()
dtc = DecisionTreeClassifier(max_depth=5)
lrc = LogisticRegression(solver='liblinear', penalty='l1')  # FIX: 'l1', not '11'
rfc = RandomForestClassifier(n_estimators=50, random_state=2)
abc = AdaBoostClassifier(n_estimators=50, random_state=2)
bc = BaggingClassifier(n_estimators=50, random_state=2)
etc = ExtraTreesClassifier(n_estimators=50, random_state=2)
gbdt = GradientBoostingClassifier(n_estimators=50, random_state=2)
xgb = XGBClassifier(n_estimators=50, random_state=2)


clfs = {
    'SVC'      : svc,
    'KN'       : knc,
    'NB'       : mnb,
    'DT'       : dtc,
    'LR'       : lrc,
    'RF'       : rfc,
    'AdaBoost' : abc,
    'BgC'      : bc,
    'ETC'      : etc,
    'GBDT'     : gbdt,
    'XGB'      : xgb   # âœ… Added proper key name
}



def train_classifierr(clf, x_train, x_test, y_train, y_test):
    clf.fit(x_train, y_train)
    y_pred = clf.predict(x_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')  # Use average param for multi-class
    return accuracy, precision


train_classifierr(svc, x_train, x_test, y_train, y_test)


accuracy_scores = []
precision_scores = []

for name, clf in clfs.items():
    acc, prec = train_classifierr(clf, x_train, x_test, y_train, y_test)
    
    print(f"For {name}:")
    print(f"  Accuracy  = {acc:.4f}")
    print(f"  Precision = {prec:.4f}")
    
    accuracy_scores.append(acc)
    precision_scores.append(prec)



performance_df=pd.DataFrame({"Algorithm":clfs.keys(),"Accuracy":accuracy_scores,"Precison":precision_scores}).sort_values("Precison",ascending=False)


performance_df


performance_df22 = pd.melt (performance_df, id_vars = "Algorithm")
performance_df22


sns.catplot(data=performance_df22,x="Algorithm", y="value", hue="variable",kind="bar",height=5)
plt.ylim(0.5,1.0)
plt.xticks(rotation='vertical')
plt.show()


# model improve
# 1. change the max_features parameter of tfIdf


performance_df24=pd.DataFrame({"Algorithm":clfs.keys(),"Accuracy_max_ft_3000":accuracy_scores,"Precison_max_fit_3000":precision_scores}).sort_values("Precison_max_fit_3000",ascending=False)



