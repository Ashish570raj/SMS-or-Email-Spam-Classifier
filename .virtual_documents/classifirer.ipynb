import pandas as pd
import numpy as np


df = pd.read_csv("spam.csv", encoding='latin1')


df.head()


df.shape


# 1. Data Cleaning
# 2. EDA
# 3. Text Preprocessing
# 4. MOdel building
# 5. Evaluation
# 6. Improvement
# 7. Website
# 8. Deploy





df.info()


# drop last 3 columns
df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace =True)


df.head()


#renaming the columns
df.rename(columns={'v1':'Target','v2':'text'},inplace=True)


df.head()


from sklearn.preprocessing import LabelEncoder
encoder=LabelEncoder()


df['Target']=encoder.fit_transform(df['Target'])


df.head()


#missing values
df.isnull().sum()


df.duplicated().sum()


df.drop_duplicates(keep='first')


df=df.drop_duplicates(keep='first')


df.duplicated().sum()


df.shape





df.head()


df['Target'].value_counts()


import matplotlib.pyplot as plt


plt.pie(df['Target'].value_counts(),labels=['ham','spam'],autopct="%0.2f")
plt.show()





import nltk


nltk.download('punkt_tab')


df['text'].apply(len)


df['num_characters']=df['text'].apply(len)


df.head()


#num of words
df['num_words']=df['text'].apply(lambda x:len(nltk.word_tokenize(x)))


df.head()


df['text'].apply(lambda x:nltk.sent_tokenize(x))


df['num_sentences']=df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))


df.head()


df[['num_characters','num_words','num_sentences']].describe()


#for ham messeges
df[df['Target']==0][['num_characters','num_words','num_sentences']].describe()


#spam
df[df['Target']==1][['num_characters','num_words','num_sentences']].describe()


import seaborn as sns


sns.histplot(df[df['Target']==0]['num_characters'])
sns.histplot(df[df['Target']==1]['num_characters'],color='red')


sns.histplot(df[df['Target']==0]['num_words'])
sns.histplot(df[df['Target']==1]['num_words'],color='red')


print(df.columns)



sns.pairplot(data=df,hue='Target')


print(df.dtypes)



# Select only numeric columns
numeric_df = df.select_dtypes(include=['int64', 'int32', 'float64'])

# Plot heatmap
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix (Numeric Features Only)")
plt.show()





def transform_text(text):
    text= text.lower()
    text= nltk.word_tokenize(text)
    return text


transform_text("HI how ARE You")


df['text']


df['text'][0]



def transform_text(text):
    text= text.lower()
    text= nltk.word_tokenize(text)
    
    y=[]
    for i in text:
        if i.isalnum():
            y.append(i)

    return y


transform_text("hi HOW are you@#")


import nltk
nltk.download('stopwords')


from nltk.corpus import stopwords
stopwords.words('english')


import string
string.punctuation


def transform_text(text):
    text= text.lower()
    text= nltk.word_tokenize(text)
    
    y=[]
    for i in text:
        if i.isalnum():
            y.append(i)

    text =y[:]
    y.clear()
    
    for i in text:
        if i not in stopwords.words('english') and i not in string.punctuation:
            y.append(i)
            
    return y


transform_text("hi how are you AshiSh ##@")


transform_text("Did you like my presentation on ML?")


df['text'][2000]


from nltk.stem.porter import PorterStemmer


ps=PorterStemmer()
ps.stem('loving')


def transform_text(text):
    text= text.lower()
    text= nltk.word_tokenize(text)
    
    y=[]
    for i in text:
        if i.isalnum():
            y.append(i)

    text =y[:]
    y.clear()
    
    for i in text:
        if i not in stopwords.words('english') and i not in string.punctuation:
            y.append(i)
            
    text=y[:]
    y.clear()
    
    for i in text:
        y.append(ps.stem(i))
            
    return " ".join(y)


transform_text("I loved the YT lectures On MAchine Learning. HOw about you?")


df['text'][0]


transform_text("'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'")


df['transformed_text']=df['text'].apply(transform_text)


df.head()


from wordcloud import WordCloud
wc= WordCloud(width=500,height=500,min_font_size=10,background_color='white')


spam_wc= wc.generate(df[df['Target']==1]['transformed_text'].str.cat(sep=' '))
plt.imshow(spam_wc)



ham_wc= wc.generate(df[df['Target']==0]['transformed_text'].str.cat(sep=' '))
plt.imshow(ham_wc)



df.head()


spam_corpus=[]
for msg in df[df['Target']==1]['transformed_text'].to_list():
    for words in msg.split():
        spam_corpus.append(words)


len(spam_corpus)


from collections import Counter
top_words_spam=Counter(spam_corpus).most_common(30)
top_words_spam


dff=pd.DataFrame(top_words_spam,columns=['words','counts'])
dff


sns.barplot(x='words', y='counts', data=dff)
plt.xticks(rotation='vertical')
plt.show()


df.head()


ham_corpus=[]
for msg in df[df['Target']==0]['transformed_text'].to_list():
    for words in msg.split():
        ham_corpus.append(words)


len(ham_corpus)


from collections import Counter
top_words_ham=Counter(ham_corpus).most_common(30)
top_words_ham


dfff=pd.DataFrame(top_words_ham,columns=['words','counts'])
dfff


sns.barplot(x='words', y='counts', data=dfff)
plt.xticks(rotation='vertical')
plt.show()








df.head()


from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
cv= CountVectorizer()
tfidf=TfidfVectorizer(max_features=3000)


x= tfidf.fit_transform(df['transformed_text']).toarray()


x.shape


y=df['Target'].values


y


from sklearn.model_selection import train_test_split


x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.2,random_state=2)


from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score


gnb=GaussianNB()
mnb=MultinomialNB()
bnb=BernoulliNB()


gnb.fit(x_train,y_train)
y_pred1=gnb.predict(x_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))


mnb.fit(x_train,y_train)
y_pred2=mnb.predict(x_test)
print(accuracy_score(y_test,y_pred2))
print(confusion_matrix(y_test,y_pred2))
print(precision_score(y_test,y_pred2))


bnb.fit(x_train,y_train)
y_pred3=bnb.predict(x_test)
print(accuracy_score(y_test,y_pred3))
print(confusion_matrix(y_test,y_pred3))
print(precision_score(y_test,y_pred3))


### we will choose tfidf and mnb


from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
# from sklearn.svm import SVC
# from sklearn.tree import DecisionTreeClassifier
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.ensemble import AdaBoostClassifier
# from sklearn.ensemble import BaggingClassifier
# from sklearn.ensemble import ExtraTreesClassifier
# from sklearn.ensemble import GradientBoostingClassifier


mnb = MultinomialNB()
lrc = LogisticRegression(solver='liblinear', penalty='l1')  # FIX: 'l1', not '11'
rfc = RandomForestClassifier(n_estimators=50, random_state=2)
xgb = XGBClassifier(n_estimators=50, random_state=2)
# # Creating classifier objects with parameters
# svc = SVC(kernel='sigmoid', gamma=1.0)
# knc = KNeighborsClassifier()
# dtc = DecisionTreeClassifier(max_depth=5)
# abc = AdaBoostClassifier(n_estimators=50, random_state=2)
# bc = BaggingClassifier(n_estimators=50, random_state=2)
# etc = ExtraTreesClassifier(n_estimators=50, random_state=2)
# gbdt = GradientBoostingClassifier(n_estimators=50, random_state=2)


clfs = {
    'NB'       : mnb,
    'LR'       : lrc,
    'RF'       : rfc,
    'XGB'      : xgb   # âœ… Added proper key name
#     'SVC'      : svc,
#     'KN'       : knc,
#     'DT'       : dtc,
#     'AdaBoost' : abc,
#     'BgC'      : bc,
#     'ETC'      : etc,
#     'GBDT'     : gbdt,
# 
}



def train_classifierr(clf, x_train, x_test, y_train, y_test):
    clf.fit(x_train, y_train)
    y_pred = clf.predict(x_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')  # Use average param for multi-class
    return accuracy, precision


train_classifierr(lrc, x_train, x_test, y_train, y_test)


accuracy_scores = []
precision_scores = []

for name, clf in clfs.items():
    acc, prec = train_classifierr(clf, x_train, x_test, y_train, y_test)
    
    print(f"For {name}:")
    print(f"  Accuracy  = {acc:.4f}")
    print(f"  Precision = {prec:.4f}")
    
    accuracy_scores.append(acc)
    precision_scores.append(prec)






performance_df=pd.DataFrame({"Algorithm":clfs.keys(),"Accuracy":accuracy_scores,"Precison":precision_scores}).sort_values("Precison",ascending=False)


performance_df


performance_df1= pd.melt (performance_df, id_vars = "Algorithm")
performance_df1


sns.catplot(data=performance_df1,x="Algorithm", y="value", hue="variable",kind="bar",height=5)
plt.ylim(0.5,1.0)
plt.xticks(rotation='vertical')
plt.show()


# model improve
# 1. change the max_features parameter of tfIdf


temp_df=pd.DataFrame({"Algorithm":clfs.keys(),"Accuracy_max_ft_3000":accuracy_scores,"Precison_max_fit_3000":precision_scores}).sort_values("Precison_max_fit_3000",ascending=False)


performance_df.merge(temp_df,on='Algorithm')














from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
cv= CountVectorizer()
tfidf=TfidfVectorizer(max_features=3000)


x= tfidf.fit_transform(df['transformed_text']).toarray()



x
# so this output contain numbers somewere between
# so we can apply scalling


# from sklearn.preprocessing import MinMaxScaler
# scaler= MinMaxScaler()
# x=scaler.fit_transform(x)


# appending the num_characters col to x
x=np.hstack((x,df['num_characters'].values.reshape(-1,1)))


x.shape


y=df['Target'].values



from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.2,random_state=2)


from sklearn.metrics import accuracy_score,confusion_matrix,precision_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier


mnb = MultinomialNB()
lrc = LogisticRegression(solver='liblinear', penalty='l1')  # FIX: 'l1', not '11'
rfc = RandomForestClassifier(n_estimators=50, random_state=2)
xgb = XGBClassifier(n_estimators=50, random_state=2)


def train_classifierr(clf, x_train, x_test, y_train, y_test):
    clf.fit(x_train, y_train)
    y_pred = clf.predict(x_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')  # Use average param for multi-class
    return accuracy, precision


accuracy_scores = []
precision_scores = []

for name, clf in clfs.items():
    acc, prec = train_classifierr(clf, x_train, x_test, y_train, y_test)
    
    print(f"For {name}:")
    print(f"  Accuracy  = {acc:.4f}")
    print(f"  Precision = {prec:.4f}")
    
    accuracy_scores.append(acc)
    precision_scores.append(prec)



temp_df2=pd.DataFrame({"Algorithm":clfs.keys(),"Accuracy_scaling":accuracy_scores,"Precison_scaling":precision_scores}).sort_values("Precison_scaling",ascending=False)


new_df=performance_df.merge(temp_df,on='Algorithm')


new_scaled_df=new_df.merge(temp_df2,on='Algorithm')
new_scaled_df



temp_df3=pd.DataFrame({"Algorithm":clfs.keys(),"Accuracy_num_char":accuracy_scores,"Precison_num_char":precision_scores}).sort_values("Precison_num_char",ascending=False)


new_num_char_df=new_scaled_df.merge(temp_df3,on='Algorithm')
new_num_char_df


from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
cv= CountVectorizer()
tfidf=TfidfVectorizer(max_features=3000)


x= tfidf.fit_transform(df['transformed_text']).toarray()



x.shape


y=df['Target'].values



from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.2,random_state=2)


from sklearn.ensemble import VotingClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier




# Define individual classifiers
nb = MultinomialNB()
rf = RandomForestClassifier(n_estimators=100, random_state=42)
xgb = XGBClassifier(eval_metric='logloss', random_state=42)




# Voting Classifier (soft voting recommended for probability-based models)
voting_clf = VotingClassifier(
    estimators=[
        ('nb', nb),
        ('rf', rf),
        ('xgb', xgb)
    ],
    voting='soft'  # Use 'hard' for majority class voting
)





# Fit on training data
voting_clf.fit(x_train, y_train)




# Predict on test data
y_pred = voting_clf.predict(x_test)




# Evaluate
from sklearn.metrics import accuracy_score, precision_score




accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)

print(f"Voting Classifier Accuracy: {accuracy:.4f}")
print(f"Voting Classifier Precision: {precision:.4f}")






import pickle
pickle.dump(tfidf,open('vectorizer.pkl','wb'))
pickle.dump(voting_clf,open('model.pkl','wb'))

































